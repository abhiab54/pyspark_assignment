{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146dc555-8201-496b-bb40-8d7ac96a1112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file1.csv']\n",
      "/data/file1.csv\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- timestamp: string (nullable = false)\n",
      " |-- batch_uuid: string (nullable = false)\n",
      "\n",
      "None\n",
      "DataFrame[id: string, name: string, age: string, timestamp: string, batch_uuid: string]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp, row_number, date_format\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "\n",
    "# define the schema for incoming data\n",
    "SCHEMA = StructType(\n",
    "[\n",
    "    StructField('id', StringType(), True), \n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', StringType(), True), \n",
    "]\n",
    ")\n",
    "\n",
    "def add_fields(df):\n",
    "    # Add batch_id and current timestamp columns\n",
    "    df = df.withColumn(\"timestamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    df = df.withColumn(\"batch_uuid\", lit(uuid.uuid4().hex))\n",
    "    return df\n",
    "    \n",
    "def start_spark_history_server(log_dir,event_dir):\n",
    "    \"\"\"\n",
    "    Launches a Spark History Server and configures it to read logs from the specified directory.\n",
    "    \"\"\"\n",
    "    # Set Spark configuration\n",
    "    conf = SparkConf().setAppName(\"SparkHistoryServer\").set(\"spark.eventLog.enabled\", \"true\") \\\n",
    "                      .set(\"spark.eventLog.dir\", event_dir).set(\"spark.history.fs.logDirectory\", log_dir)\n",
    "\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "    # Start Spark History Server\n",
    "    os.system(f\"nohup spark-submit --class org.apache.spark.deploy.history.HistoryServer \\\n",
    "            $SPARK_HOME/jars/spark-*.jar > /dev/null 2>&1 &\")\n",
    "\n",
    "    return spark\n",
    "    \n",
    "class IngestionJob:\n",
    "    def __init__(self, spark, log_file):\n",
    "        self.spark = spark\n",
    "\n",
    "        # Initialize logger\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # Set up file handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "\n",
    "        # Set log message format\n",
    "        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        # Add file handler to logger\n",
    "        self.logger.addHandler(file_handler)\n",
    "        \n",
    "    def read_csv(self, SCHEMA, file_path):\n",
    "\n",
    "    # read each csv file into a dataframe. We will consider all files are without header\n",
    "        df = self.spark\\\n",
    "        .read.format(\"csv\")\\\n",
    "        .option(\"delimiter\", \"|\")\\\n",
    "        .option(\"header\", \"false\")\\\n",
    "        .option(\"encoding\", \"ISO-8859-1\")\\\n",
    "        .schema(SCHEMA)\\\n",
    "        .load(file_path)\n",
    "\n",
    "    # now if any files happens to have a header then we can just remove that header line\n",
    "        row1 = [i for i in df.head(1)[0].asDict().values()] # get first row\n",
    "        schema_list = [(x.name) for x in SCHEMA.fields] # get schema as list\n",
    "        \n",
    "        if row1 == schema_list: # if first row is the schema then remove that row\n",
    "            row1 = df.limit(1)\n",
    "            df = df.subtract(row1)   \n",
    "        \n",
    "        print(file_path)\n",
    "        self.logger.info(f\"Read CSV file with {df.count()} rows from {file_path}\")\n",
    "\n",
    "        return df\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def ingest_data(self, df, output_path):\n",
    "        \n",
    "        # Write to Delta Lake with append mode and partition by batch_id and timestamp\n",
    "        output_table = f\"{output_path}\"\n",
    "        df.write.format(\"delta\").mode(\"append\").save(output_table)\n",
    "        self.logger.info(f\"Added batch_id and timestamp columns to DataFrame\")\n",
    "        return df\n",
    "        \n",
    "    def ingest_csv_to_deltalake(self, file_path, output_path):\n",
    "    # define the schema for incoming data\n",
    "        SCHEMA = StructType(\n",
    "        [\n",
    "            StructField('id', StringType(), True), \n",
    "            StructField('name', StringType(), True),\n",
    "            StructField('age', StringType(), True), \n",
    "        ]\n",
    "        )\n",
    "    # read each csv file into a dataframe. We will consider all files are without header\n",
    "        df = self.spark\\\n",
    "        .read.format(\"csv\")\\\n",
    "        .option(\"delimiter\", \"|\")\\\n",
    "        .option(\"header\", \"false\")\\\n",
    "        .option(\"encoding\", \"ISO-8859-1\")\\\n",
    "        .schema(SCHEMA)\\\n",
    "        .load(file_path)\n",
    "    \n",
    "    # now if any files happens to have a header then we can just remove that header line\n",
    "        row1 = [i for i in df.head(1)[0].asDict().values()] # get first row\n",
    "        schema_list = [(x.name) for x in SCHEMA.fields] # get schema as list\n",
    "        \n",
    "        if row1 == schema_list: # if first row is the schema then remove that row\n",
    "            row1 = df.limit(1)\n",
    "            df = df.subtract(row1)\n",
    "            \n",
    "        print(file_path)\n",
    "        self.logger.info(f\"Read CSV file with {df.count()} rows from {file_path}\")\n",
    "\n",
    "        # Add batch_id and current timestamp columns\n",
    "        df = df.withColumn(\"timestamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "        df = df.withColumn(\"batch_uuid\", lit(uuid.uuid4().hex))\n",
    "        self.logger.info(f\"Added batch_id and timestamp columns to DataFrame\")\n",
    "        \n",
    "        \n",
    "\n",
    "        # Write to Delta Lake with append mode and partition by batch_id and timestamp\n",
    "        output_table = f\"{output_path}\"\n",
    "        df.write.format(\"delta\").mode(\"append\").save(output_table)\n",
    "        self.logger.info(f\"Wrote {df.count()} rows to Delta Lake at {output_table}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize SparkSession\n",
    "    spark = SparkSession.builder.appName(\"IngestionJob\").getOrCreate()\n",
    "\n",
    "    # Parse arguments\n",
    "    # parser = argparse.ArgumentParser(description='Ingest CSV files into Delta Lake')\n",
    "    # parser.add_argument(\"--data_path\", help=\"Path to csv files\", required=True)\n",
    "    # parser.add_argument('--output_path', type=str, default='delta', help='Output path for Delta Lake table')\n",
    "    # parser.add_argument('--log_file', type=str, default='ingestion.log', help='Log file path')\n",
    "    # parser.add_argument('--event_dir', type=str, default='/events', help='Event directory')\n",
    "    # args = parser.parse_args()\n",
    "    data_path = '/data'\n",
    "    output_path = '/output/delta'\n",
    "    log_file = '/logs/ingestion.log'\n",
    "    event_dir = '/data/events'\n",
    "\n",
    "    # create empty dataframe\n",
    "    # Create an empty RDD\n",
    "    emp_RDD = spark.sparkContext.emptyRDD()\n",
    "     \n",
    "    # Create empty schema\n",
    "    columns = StructType([])\n",
    "     \n",
    "    # Create an empty RDD with empty schema\n",
    "    final_df = spark.createDataFrame(data = emp_RDD,\n",
    "                                 schema = SCHEMA)\n",
    "    # Initialize IngestionJob\n",
    "    job = IngestionJob(spark, log_file)\n",
    "\n",
    "    files = os.listdir(data_path)\n",
    "    print(files)\n",
    "    file_paths = [file for file in files if file.endswith('.csv')]\n",
    "    # Process each CSV file\n",
    "    for file_path in file_paths:\n",
    "        df = job.read_csv(SCHEMA, data_path + \"/\" + file_path)\n",
    "        final_df = final_df.union(df)\n",
    "    \n",
    "    \n",
    "    final_df = add_fields(final_df)\n",
    "    print(final_df.printSchema())\n",
    "    # job.ingest_csv_to_deltalake(args.data_path + \"/\" + file_path, args.output_path)\n",
    "    # job.ingest_data(final_df, output_path)\n",
    "    print(final_df.limit(1))\n",
    "    # Stop SparkSession\n",
    "    # spark.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3199b5dd-5ba5-4b8b-bba0-df9c9f55f669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------------+--------------------+\n",
      "| id|   name|age|          timestamp|          batch_uuid|\n",
      "+---+-------+---+-------------------+--------------------+\n",
      "|  1|  Alice| 25|2023-05-11 11:17:23|f8c56d8a8b6e4e1d9...|\n",
      "|  2|    Bob| 30|2023-05-11 11:17:23|f8c56d8a8b6e4e1d9...|\n",
      "|  3|Charlie| 35|2023-05-11 11:17:23|f8c56d8a8b6e4e1d9...|\n",
      "+---+-------+---+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4998f761-5ffc-4eab-a34e-e0207512199e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8377060215b0443b87041c799c413636\n",
      "2023-05-11 11:17:236\n"
     ]
    }
   ],
   "source": [
    "def get_uuid():\n",
    "    return '8377060215b0443b87041c799c413636'\n",
    "def get_cur_ts():\n",
    "    return '2023-05-11 11:17:236'\n",
    "print(get_uuid())\n",
    "print(get_cur_ts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49fc38fd-f898-4978-9446-c225641eae5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_etl (__main__.SparkETLTestCase) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_etl (__main__.SparkETLTestCase)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/unittest/mock.py\", line 1376, in patched\n",
      "    with self.decoration_helper(patched,\n",
      "  File \"/opt/conda/lib/python3.10/contextlib.py\", line 135, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/opt/conda/lib/python3.10/unittest/mock.py\", line 1358, in decoration_helper\n",
      "    arg = exit_stack.enter_context(patching)\n",
      "  File \"/opt/conda/lib/python3.10/contextlib.py\", line 492, in enter_context\n",
      "    result = _cm_type.__enter__(cm)\n",
      "  File \"/opt/conda/lib/python3.10/unittest/mock.py\", line 1431, in __enter__\n",
      "    self.target = self.getter()\n",
      "  File \"/opt/conda/lib/python3.10/unittest/mock.py\", line 1618, in <lambda>\n",
      "    getter = lambda: _importer(target)\n",
      "  File \"/opt/conda/lib/python3.10/unittest/mock.py\", line 1257, in _importer\n",
      "    thing = __import__(import_path)\n",
      "ModuleNotFoundError: No module named 'add_fields'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.283s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "from unittest import mock\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit, current_timestamp, row_number, date_format\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from freezegun import freeze_time\n",
    "import uuid\n",
    "import os\n",
    "# import nbimporter\n",
    "\n",
    "def add_fields(df):\n",
    "    # Add batch_id and current timestamp columns\n",
    "    df = df.withColumn(\"timestamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    df = df.withColumn(\"batch_uuid\", lit(uuid.uuid4().hex))\n",
    "    return df\n",
    "\n",
    "\n",
    "class SparkETLTestCase(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = (SparkSession\n",
    "                     .builder\n",
    "                     .master(\"local[*]\")\n",
    "                     .appName(\"Unit-tests\")\n",
    "                     .getOrCreate())\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        cls.spark.stop()\n",
    "        \n",
    "    def get_uuid():\n",
    "        return uuid.uuid4().hex\n",
    "\n",
    "    @mock.patch(\"add_fields.uuid.uuid4\")\n",
    "    @freeze_time(\"2023-01-01\")\n",
    "    def test_etl(self, mock_uuid):\n",
    "        mock_uuid.return_value  = '8377060215b0443b87041c799c413636'\n",
    "        input_schema = StructType(\n",
    "        [\n",
    "            StructField('id', StringType(), True), \n",
    "            StructField('name', StringType(), True),\n",
    "            StructField('age', StringType(), True), \n",
    "        ]\n",
    "        )\n",
    "\n",
    "        #1. Prepare an input data frame that mimics our source data.\n",
    "        input_data = [(1, \"Steve\", 35)]\n",
    "        input_df = self.spark.createDataFrame(data=input_data, schema=input_schema)\n",
    "\n",
    "        #2. Prepare an expected data frame which is the output that we expect.    \n",
    "        expected_schema = StructType([\n",
    "            StructField('id', StringType(), True), \n",
    "            StructField('name', StringType(), True),\n",
    "            StructField('age', StringType(), True), \n",
    "            StructField('timestamp', StringType(), True), \n",
    "            StructField('batch_uuid', StringType(), True), \n",
    "                ])\n",
    "        expected_data = [(1, \"Steve\", 35, '2023-05-11 11:17:236', '8377060215b0443b87041c799c413636') ]\n",
    "        expected_df = self.spark.createDataFrame(data=expected_data, schema=expected_schema)\n",
    "        \n",
    "        #3. Apply our transformation to the input data frame\n",
    "        output_df = add_fields(input_df)               \n",
    "        \n",
    "        #4. Assert the output of the transformation to the expected data frame.\n",
    "        self.assertEqual(sorted(expected_df.collect()), sorted(output_df.collect()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main = SparkETLTestCase()\n",
    "\n",
    "    # This executes the unit test/(itself)\n",
    "    import sys\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(SparkETLTestCase)\n",
    "    unittest.TextTestRunner(verbosity=4,stream=sys.stderr).run(suite)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59eeffae-507a-4aa9-978b-ff4a1dcbf1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'home'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjoyan\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_fields\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'home'"
     ]
    }
   ],
   "source": [
    "from home.joyan.code.prog import add_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c238c2b-8e04-41e1-a8f7-83f800338b26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot set 'today' attribute of immutable type 'datetime.date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/unittest/mock.py:1556\u001b[0m, in \u001b[0;36m_patch.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;43msetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot set 'today' attribute of immutable type 'datetime.date'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mock\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mock\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mobject(dt\u001b[38;5;241m.\u001b[39mdate, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoday\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_value\u001b[38;5;241m=\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m29\u001b[39m)):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mtoday())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/unittest/mock.py:1569\u001b[0m, in \u001b[0;36m_patch.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m-> 1569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/unittest/mock.py:1575\u001b[0m, in \u001b[0;36m_patch.__exit__\u001b[0;34m(self, *exc_info)\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Undo the patch.\"\"\"\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_local \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_original \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEFAULT:\n\u001b[0;32m-> 1575\u001b[0m     \u001b[38;5;28;43msetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_original\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot set 'today' attribute of immutable type 'datetime.date'"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from unittest import mock\n",
    "\n",
    "with mock.patch.object(dt.date, \"today\", return_value=dt.date(2020, 2, 29)):\n",
    "    print(dt.date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7dcb8-ccba-439e-86ae-b65717af457a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
